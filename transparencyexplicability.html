<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Transparency and explicability &#8212; Trustworthy AI  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Diversity, non discrimination and fairness" href="diversitynondiscrimination.html" />
    <link rel="prev" title="Technical robustness and security" href="robustnesssafety.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          Trustworthy AI</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Documentation <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="human.html">Human oversight</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacydatagov.html">Privacy and data governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="robustnesssafety.html">Technical robustness and security</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Transparency and explicability</a></li>
<li class="toctree-l1"><a class="reference internal" href="diversitynondiscrimination.html">Diversity, non discrimination and fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="environmentalsocietal.html">Environmental and societal well-being</a></li>
<li class="toctree-l1"><a class="reference internal" href="accountability.html">Accountability</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Transparency and explicability</a><ul>
<li><a class="reference internal" href="#communication">Communication</a></li>
<li><a class="reference internal" href="#ai-contributor-s-management">AI contributor’s management</a></li>
<li><a class="reference internal" href="#ai-explicability">AI explicability</a><ul>
<li><a class="reference internal" href="#statistical-methods">Statistical methods</a></li>
<li><a class="reference internal" href="#ml-based-algorithms">ML-based algorithms</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ai-transparency">AI transparency</a></li>
<li><a class="reference internal" href="#appendix-recommendations-from-the-eu">Appendix - Recommendations from the EU</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="robustnesssafety.html" title="Previous Chapter: Technical robustness and security"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Technical rob...</span>
    </a>
  </li>
  <li>
    <a href="diversitynondiscrimination.html" title="Next Chapter: Diversity, non discrimination and fairness"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Diversity, no... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="transparency-and-explicability">
<h1>Transparency and explicability<a class="headerlink" href="#transparency-and-explicability" title="Permalink to this headline">¶</a></h1>
<p>Transparency and explicability are two fundamental and distinct notions when it comes to AI trustworthiness. While explicability aims at understanding the logic behind an algorithm, transparency aims at making “public” (totally or partially - within the scope of an organisation e.g.) the AI system and anything that lives with it (data, …).</p>
<section id="communication">
<h2>Communication<a class="headerlink" href="#communication" title="Permalink to this headline">¶</a></h2>
<p>AI contributors should always clearly and timely communicate results to stakeholders. Not only they must make them understandable for technical people, but also and more importantly for non-technical people. Practically, the AI contributor must:</p>
<ul class="simple">
<li><p>Ask for feedbacks from non-technical sparring partner(s) before spreading results: is the methodology clear? Are the modelling choices (data processing, algorithms) clearly stated?</p></li>
<li><p>Keep a faithful representation of results and do not give in to confirmation bias</p></li>
<li><p>Make an outreach effort regarding AI algorithms: clearly state AI models assumptions, their advantages and potential pitfalls so that decision makers can take informed decisions. Example: qualify results of a facial recognition algorithm that would be based (pretrained) on the ImageNet dataset, which is now well known for containing bias and errors (see this <a class="reference external" href="https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/">link</a> e.g.)</p></li>
</ul>
</section>
<section id="ai-contributor-s-management">
<h2>AI contributor’s management<a class="headerlink" href="#ai-contributor-s-management" title="Permalink to this headline">¶</a></h2>
<p>AI contributors should always be put in a favourable condition by the management to give the alert in case of fraudulent, illegal, illegitimate, discriminatory or unethical results. Example: use of legitimate information when it comes to consumer scoring (in marketing e.g.).</p>
<p>A clear and shared escalation process should be defined upon a new AI system implementation in agreement with all stakeholders’ initiative. Escalations should be put in writing and documented. In case of potential breach of trust, the escalation process could take the following form:</p>
<ol class="arabic simple">
<li><p>Report the alert to the project’s team</p></li>
<li><p>If no arbitrage could be made, report the alert to the project’s sponsor &amp; management</p></li>
<li><p>If no arbitrage could be made, report the alert to the Data Protection Officer &amp; legal departments</p></li>
</ol>
</section>
<section id="ai-explicability">
<h2>AI explicability<a class="headerlink" href="#ai-explicability" title="Permalink to this headline">¶</a></h2>
<p>AI explicability aims at creating numerical methods to extract the logics behind an AI system’s decision.</p>
<p>AI contributors must be sensitised and extensively leverage machine learning interpretability literature to:</p>
<ul class="simple">
<li><p>Be able to extensively stress test model’s predictions.</p></li>
<li><p>Empower business users and foster machine learning adoption.</p></li>
<li><p>Understand results output by an AI system (characteristics &amp; information that were leveraged and which ones play an important role, decision frontier of the system, behavior of internal blocks of the AI system, like hidden layers of neural network e.g., …) and trace decisions made by an AI system over time.</p></li>
</ul>
<p>That being said, AI contributors should master a various set of tools, depending on the AI system that is implemented.</p>
<section id="statistical-methods">
<h3>Statistical methods<a class="headerlink" href="#statistical-methods" title="Permalink to this headline">¶</a></h3>
<p>AI contributors must deeply understand how to interpret coefficients and leverage econometrics-based models, like linear / logistic regression, panel data, instrumental variables, censored data, selection model, …</p>
<p>Example with linear regression coefficients:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Target</p></th>
<th class="head"><p>Feature</p></th>
<th class="head"><p>Math. interpretation</p></th>
<th class="head"><p>Plain interpretation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>level - level</p></td>
<td><p><span class="math notranslate nohighlight">\(y\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\Delta y = \beta_1 \Delta x\)</span></p></td>
<td><p>A change of 1 in <span class="math notranslate nohighlight">\(x\)</span> is expected to produce a change of <span class="math notranslate nohighlight">\(\beta_1\)</span> in <span class="math notranslate nohighlight">\(y\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>level - log</p></td>
<td><p><span class="math notranslate nohighlight">\(y\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(log(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\Delta y = (\beta_1 / 100)\% \Delta x\)</span></p></td>
<td><p>A change of 1% in <span class="math notranslate nohighlight">\(x\)</span> is expected to produce a change of <span class="math notranslate nohighlight">\(\beta_1 / 100\)</span> in <span class="math notranslate nohighlight">\(y\)</span></p></td>
</tr>
<tr class="row-even"><td><p>log - level (semi-elasticity)</p></td>
<td><p><span class="math notranslate nohighlight">\(log(y)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\% \Delta y = 100\beta_1 \Delta x\)</span> (approx. of the exact one: <span class="math notranslate nohighlight">\(\% \Delta y = 100(e^{\beta_1} -1) \Delta x\)</span>)</p></td>
<td><p>A change of 1 in <span class="math notranslate nohighlight">\(x\)</span> is expected to produce a change of <span class="math notranslate nohighlight">\(100 \beta_1 \%\)</span> in <span class="math notranslate nohighlight">\(y\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>log - log (elasticity)</p></td>
<td><p><span class="math notranslate nohighlight">\(log(y)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(log(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\% \Delta y = \beta_1 \% \Delta x\)</span></p></td>
<td><p>A change of 1% in <span class="math notranslate nohighlight">\(x\)</span> is expected to produce a change of <span class="math notranslate nohighlight">\(\beta_1 \%\)</span> in <span class="math notranslate nohighlight">\(y\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="ml-based-algorithms">
<h3>ML-based algorithms<a class="headerlink" href="#ml-based-algorithms" title="Permalink to this headline">¶</a></h3>
<p><strong>Standard interpretability</strong></p>
<p>AI contributors must properly leverage state-of-the-art methodologies for machine learning-based models. Resources lie among: <a class="reference external" href="https://github.com/slundberg/shap">Shap</a>, <a class="reference external" href="https://github.com/marcotcr/lime">LIME</a>, <a class="reference external" href="https://github.com/MAIF/shapash">Shapash</a>.</p>
<p>Below are some examples of metrics, KPIs or representations:</p>
<a class="reference internal image-reference" href="_images/interpretability.png"><img alt="_images/interpretability.png" class="align-center" src="_images/interpretability.png" style="width: 70%;" /></a>
<p>[1] <a class="reference external" href="https://stats.stackexchange.com/questions/501039/can-i-scale-and-then-interpret-shap-values-as-percent-contribution-to-the-predic">https://stats.stackexchange.com/questions/501039/can-i-scale-and-then-interpret-shap-values-as-percent-contribution-to-the-predic</a></p>
<p>It is important to study both global interpretation (at model level) and local interpretation (at individual level). This balance is the only way to achieve an exhaustive understanding of the AI system’s behavior, which in turns unlocks trust in the AI system.</p>
<a class="reference internal image-reference" href="_images/interpretability_split.png"><img alt="_images/interpretability_split.png" class="align-center" src="_images/interpretability_split.png" style="width: 50%;" /></a>
<center><i>Source: Explanatory Model Analaysis, Przemysiaw Blecek and Tornasz Burzykowski (Dec 2020)</i></center><p><strong>Towards prescriptive analysis</strong></p>
<p>Not only machine learning interpretability eventually enables AI systems to be trusted by business users, but it also unlocks prescriptive machine learning. Predictions bring value, but actions taken upon new predictions bring even more value. Interpretability might be a way to prescriptive analysis.</p>
<p>Example: for a churn model, it is important to identify high-risk consumers. It is even more important to identify why they are risky, and the actions that could be taken to decrease their risk. This can be performed by producing logical and interpretable rules that describe this set of risky people. <a class="reference external" href="https://github.com/scikit-learn-contrib/skope-rules">Skope-rules</a> is a candidate (<a class="reference external" href="https://app.livestorm.co/quantmetry/meetup-ai-engineering-7-lethique-and-intelligibilite-en-application/live?s=3d1742f7-c41f-4f6e-9087-9931f39d7a16#/chat">Source</a> for the full methodology - in French).</p>
</section>
</section>
<section id="ai-transparency">
<h2>AI transparency<a class="headerlink" href="#ai-transparency" title="Permalink to this headline">¶</a></h2>
<p>While creating an AI system, AI contributors should enforce the system’s transparency, namely:</p>
<ul class="simple">
<li><p>Document the method / theory behind the AI system.</p></li>
<li><p>Document, and ideally make public (or available within the company), the implementation (code) of the AI system.</p></li>
<li><p>Document, and ideally make public (or available within the company), the data the AI system was trained on.</p></li>
</ul>
<p>Transparency allows to challenge the AI system’s weaknesses, challenge potential pitfalls of the training data, which in turns helps to check legal &amp; ethical compliance, and to protect individuals.</p>
<p>Source: <a class="reference external" href="https://amp-lepoint-fr.cdn.ampproject.org/c/s/amp.lepoint.fr/2430716">[FR] Transparence et explicabilité des algorithmes, la grande confusion</a></p>
</section>
<section id="appendix-recommendations-from-the-eu">
<h2>Appendix - Recommendations from the EU<a class="headerlink" href="#appendix-recommendations-from-the-eu" title="Permalink to this headline">¶</a></h2>
<p>Below are the recommendations directly reported from <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">EU</a>.</p>
<a class="reference internal image-reference" href="_images/transparency_1.png"><img alt="_images/transparency_1.png" class="align-center" src="_images/transparency_1.png" style="width: 60%;" /></a>
<a class="reference internal image-reference" href="_images/transparency_2.png"><img alt="_images/transparency_2.png" class="align-center" src="_images/transparency_2.png" style="width: 60%;" /></a>
<a class="reference internal image-reference" href="_images/transparency_3.png"><img alt="_images/transparency_3.png" class="align-center" src="_images/transparency_3.png" style="width: 60%;" /></a>
<ul class="simple">
<li></li>
</ul>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright Danone, Datacraft, 2021.<br/>
    </p>
  </div>
</footer>
  </body>
</html>