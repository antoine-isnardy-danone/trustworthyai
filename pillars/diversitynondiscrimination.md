# Diversity, non discrimination and fairness
AI systems must be built:
- Through an inclusive design process, so that stakeholders directly or indirectly affected by the AI system can understand and trust it. Practically: stakeholder’s feedback along the design process as well as during the run phase is necessary.
- Towards a non-discriminatory usage, namely, it must not be reserved to a subgroup of the society/entity/organisation.

AI contributors must think carefully:
- When defining class labels and target variable. Poorly defining them might have an unwanted adverse impact on specific groups.
- Not to use training data (labelling examples) with a bias
- Not to use training data (data collection) with a biased sampling procedure
- Not to select specific features that might introduce a bias against certain groups
- Not to use proxies that might strongly correlate with protected characteristics
- Not to discriminate on purpose (for example: pregnant woman)

AI contributors must foster the creation of bias-free AI systems and give themselves the means to reasonably arbitrage between performance and fairness, through the use of proper frameworks. Example:
- Anticipate bias and question the risk of discrimination:
    - In the data used to train AI systems (origin, gender, age, characteristics related to the brand image, …). Bias might come from history, incompleteness and/or bad governance data models. Any identifiable and discriminatory bias should be removed as much as possible in the data collection phase.
    - In the AI systems (eg. algorithm programming). Bias might come from poorly defined constraints, bad decisions, wrong requirements. This can be counteracted by putting in place oversight processes. [PLACEHOLDER FOR DEEP DIVE AND MORE DOC]
- Quantify the risk of breach of fairness on identified sensitive populations, through the use of metrics, including but not limited to: statistical parity difference, equal opportunity difference, average odds difference, disparate impact, theil index 
- Correct bias, through the use of algorithms, including but not limited to: reweighting, optimized pre-processing, adversarial debiasing, reject option based classification 

Practically: AI Fairness 360 by IBM, ethicalai,
| Data collection & preprocessing  | AI contributors must anticipate bias and question the risk of discrimination in the data collection and preprocessing of data (origin, gender, age, characteristics related to the brand image, …)  |<ul><li>Deeply understand the business process behind the data generation process, thanks to extensive discussion with business partners:<ul><li><b>When</b> was the data collected? (One-shot vs long period, ...)</li><li><b>Who</b> collected the data? (General-purpose team vs task-specific team)</li><li><b>What</b> was targeted? (Precise subset vs random set)</li><li><b>How</b> was the data collected? (Manually, sensors, ...)</li></ul><li>Conduct descriptive analysis (univariate analysis, multivariate analysis, statistical testing)</li><li>Document treatments</li><li>Prevent from adding unlawful or illegitimate features (example: sex for a granting score, …)</li> |
| AI system training  | While training AI models, AI contributors must carefully understand model’s underlying mechanisms, carefully calibrate them, and carefully analyze results  | <ul><li>Not only compute a single aggregated performance metrics (<a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R2</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">accuracy</a>, or any other <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">model evaluation metrics</a>…), but also carefully analyze result’s dispersion, and focus on large errors. For instance, while doing forecast, not only quantitatively refer to <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSE</a> / <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">MAPE</a> / <a href="https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error">SMAPE</a> / …, but also to qualitative measurements or representations like predictions vs reality</li><li>Leverage uncertainty for predictions. Examples: <a href="https://facebook.github.io/prophet/docs/quick_start.html">Prophet</a>, <a href="https://github.com/simai-ml/MAPIE/tree/master/mapie">MAPIE</a>, <a href="https://docs.pymc.io/">pyMC3</a></li></ul> |
| AI system industrialisation |  While industrialising AI models, AI contributors must ensure AI system's sustainability | <ul><li>Question generalization: whether training data are a relevant proxy of what is likely to happen while predicting with the trained model on new data</li><li>Implement reproducibility so that tracing back the root cause of anomalies is feasible. One solution: <a href="https://mlflow.org/">MLFlow</a></li><li>Implement feedback loops to detect / anticipate / prevent drifts in the data, to prevent new bias in the data, model’s loss of performance, through the use of monitoring metrics</li><li>Implement fallback plans to prevent damages. Examples: alerting, switch from statistical to rule-based procedures</li></ul>|






The following table shows guidelines for each phase of an AI project, so as to support AI contributors in the creation of bias-free AI systems.

## AI Project lifecycle

| Project phase  | Guideline  | Practical consequences  |
|---|---|---|
| Ideation & design  | At the start of the ideation & design phase of an AI project, AI contributors must be aware of potential discrimination risks/breaches in an AI project and avoid initiating a project that discriminates on purpose. Stakeholders should always be involved throughout the whole project and must have complete trust & understanding of the system and its risks. | <ul><li>	Be aware of discrimination sensitive AI topics, such as:<ul><li>Police, crime prevention</li><li>Selection of employees and students</li><li>Advertising (Targeting specific people based on characteristics)</li><li>Price discrimination (charge customers differently based on characteristics)</li><li>Image & Search analysis</li><li>Translation tools (human biases in translation related to gender stereotypes)</li></ul><li>AI systems must be built:<ul><li>Through an inclusive design process, so that stakeholders directly or indirectly affected by the AI system can understand and trust it. Practically: stakeholder’s feedback along the design process as well as during the run phase is necessary.</li><li>Towards a non-discriminatory usage, namely, it must not be reserved to a subgroup of the society/entity/organisation.|
| Data collection & preprocessing  | AI contributors must anticipate bias and question the risk of discrimination in the data collection and preprocessing of data (origin, gender, age, characteristics related to the brand image, …)  |<ul><li>Deeply understand the business process behind the data generation process, thanks to extensive discussion with business partners:<ul><li><b>When</b> was the data collected? (One-shot vs long period, ...)</li><li><b>Who</b> collected the data? (General-purpose team vs task-specific team)</li><li><b>What</b> was targeted? (Precise subset vs random set)</li><li><b>How</b> was the data collected? (Manually, sensors, ...)</li></ul><li>Conduct descriptive analysis (univariate analysis, multivariate analysis, statistical testing)</li><li>Document treatments</li><li>Prevent from adding unlawful or illegitimate features (example: sex for a granting score, …)</li> |
| AI system training  | While training AI models, AI contributors must carefully understand model’s underlying mechanisms, carefully calibrate them, and carefully analyze results  | <ul><li>Not only compute a single aggregated performance metrics (<a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R2</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">accuracy</a>, or any other <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">model evaluation metrics</a>…), but also carefully analyze result’s dispersion, and focus on large errors. For instance, while doing forecast, not only quantitatively refer to <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSE</a> / <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">MAPE</a> / <a href="https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error">SMAPE</a> / …, but also to qualitative measurements or representations like predictions vs reality</li><li>Leverage uncertainty for predictions. Examples: <a href="https://facebook.github.io/prophet/docs/quick_start.html">Prophet</a>, <a href="https://github.com/simai-ml/MAPIE/tree/master/mapie">MAPIE</a>, <a href="https://docs.pymc.io/">pyMC3</a></li></ul> |
| AI system industrialisation |  While industrialising AI models, AI contributors must ensure AI system's sustainability | <ul><li>Question generalization: whether training data are a relevant proxy of what is likely to happen while predicting with the trained model on new data</li><li>Implement reproducibility so that tracing back the root cause of anomalies is feasible. One solution: <a href="https://mlflow.org/">MLFlow</a></li><li>Implement feedback loops to detect / anticipate / prevent drifts in the data, to prevent new bias in the data, model’s loss of performance, through the use of monitoring metrics</li><li>Implement fallback plans to prevent damages. Examples: alerting, switch from statistical to rule-based procedures</li></ul>|

