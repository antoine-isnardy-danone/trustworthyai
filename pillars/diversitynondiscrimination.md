# Diversity, non discrimination and fairness
AI contributors must foster the creation of bias-free AI systems and give themselves the means to reasonably arbitrage between performance and fairness, through the use of proper frameworks. We will discuss an AI project lifecycle through the eyes of fairness, and explain how one can detect discrimination through statistical methods.

The following table shows guidelines for each phase of an AI project, so as to support AI contributors in the creation of bias-free AI systems.

## AI Project lifecycle

| Project phase  | Guideline  | Practical consequences  |
|---|---|---|
| Ideation & design  | At the start of the ideation & design phase of an AI project, AI contributors must be aware of potential discrimination risks/breaches in an AI project and avoid initiating a project that discriminates on purpose. Stakeholders should always be involved throughout the whole project and must have complete trust & understanding of the system and its risks. | <ul><li>	Be aware of discrimination sensitive AI topics, such as:<ul><li>Police, crime prevention</li><li>Selection of employees and students</li><li>Advertising (Targeting specific people based on characteristics)</li><li>Price discrimination (charge customers differently based on characteristics)</li><li>Image & Search analysis</li><li>Translation tools (human biases in translation related to gender stereotypes)</li></ul><li>AI systems must be built:<ul><li>Through an inclusive design process, so that stakeholders directly or indirectly affected by the AI system can understand and trust it. Practically: stakeholder’s feedback along the design process as well as during the run phase is necessary.</li><li>Towards a non-discriminatory usage, namely, it must not be reserved to a subgroup of the society/entity/organisation.|
| Data collection & preprocessing  | AI contributors must anticipate bias and question the risk of discrimination in the data collection and preprocessing of data (origin, gender, age, characteristics related to the brand image, …)  |<ul><li>Pay much attention to vulnerable demographics, for example: children, minorities, disabled persons, elderly persons, or immigrants.</li></ul><li> Bias might come from history, incompleteness and/or bad governance data models. Any identifiable and discriminatory bias should be removed as much as possible in the data collection phase.  </li><li> Be careful when defining class labels and target variable. Poorly defining them might have an unwanted adverse impact on specific groups.</li><li> Do not use proxies that might strongly correlate with protected characteristics</li><li> Do not select specific features that might introduce a bias against certain groups </li><li> Do not use data (data collection) with a biased sampling procedure </li><li> Do not use data (labelling examples) with a bias </li> |
| AI system training  | While training (programming) AI models, AI contributors must carefully understand the system its mechanisms, analyze results on fairness and correct bias where needed.  | <ul><li> Bias might come from poorly defined constraints, bad decisions, wrong requirements. This can be counteracted by putting in place oversight processes</li><li> Ensure that individuals and minority groups are free from bias.</li><li> Positives and negatives resulting from AI should be evenly distributed, avoiding to place vulnerable demographics in a position of greater vulnerability </li></ul> |
| AI system industrialisation |  While industrializing AI models, AI contributors must ensure that the AI’s system is fair and non-discriminative. Feedback loops should be put in place when the model is deployed and proper actions should be taken when there is a signal of discrimination. | <ul><li> If harm occurs, AI systems must provide users with effective redress, or effective remedy if data practices are no longer aligned with human beings’ individual or collective preferences. </li><li> AI contributors must respect high standards of accountability </li><li>Implement feedback loops to detect / anticipate / prevent drifts in the data, to prevent new bias in the data, model’s loss of performance, through the use of monitoring metrics</li><li>Implement fallback plans to prevent damages. Examples: alerting, switch from statistical to rule-based procedures</li></ul>|

The steps in the aforementioned table should be supporting the AI contributor in their project. However, some of these guidelines are subject to human opinion and unfairness might be hard to detect. Therefore, several statistical metrics exist that might help in the identification of discrimination and correction of it. The following table discusses these metrics and shows already existing packages for their implementation.

## Identification metrics and ways for correcting discrimination bias and unfairness

| Topic  | Guideline  | Metrics  | 
|---|---| ---|
| Identification  | Quantify the risk of breach of fairness on identified sensitive populations, through the use of metrics | <ul><li>	As an example, one might use some of the following metrics for identification of discrimination:<ul><li> statistical parity difference </li><li> equal opportunity difference </li><li> average odds difference </li><li> disparate impact </li><li> theil index </li></ul> |
| Correction  | After identification of bias, one might correct the bias through the use of algorithms | <ul><li> As an example, one might use some of the following methods for correction of the bias:<ul><li> reweighting </li><li> optimized pre-processing </li><li> adversarial debiasing </li><li>reject option based classification </li></ul> |
| Already existing solutions (packages)  | Some open sources packages are created that might help in detection & correction of unfairness in an AI system | <ul><li> Please have a look at the following:<ul><li> <a href="https://aif360.mybluemix.net/">AI Fairness 360 by IBM</a> </li><li> <a href="ekimetrics/ethical-ai-toolkit: Open source experiments on bias, fairness & ethical AI frameworks (github.com)"> Ethical AI Toolkit (Github):</a> </li></ul> |
