# Human oversight

As stated by the EU, "human oversight ensures that an AI system does not undermine human autonomy or cause other adverse effects".

## Guidelines depending on the AI system lifecyle

| Phase  | Guideline  | Practical consequences  |
|---|---|---|
| Conception (prior to AI system development)  | AI contributors should always assess “why” they set up an AI system prior to actually doing it | <ul><li>What is the end goal?</li><li>How much does it benefit to people? Is there a risk of attempting to fundamental rights? If so, have the said risk(s) been assessed (see section below)?</li><li>What are the potential social and environmental consequences?</li><li>Overall, did the AI system receive external clearance (external to AI practitioners about to implement it)?</li></ul>|
| Use (after AI system development) | Users of an AI system must be able to control it | <ul><li>AI contributors must make it clear for users that they interact with an AI system (in the sense that it must not be hidden) to avoid "unfair manipulation, herding[,] conditioning, [...]"</li></li><li>AI contributors must make it easy for users to:<ul><li>Interact with the AI system through clear documentation</li><li>Regularly challenge and audit the AI system (through a clear and shared operating model detailed early in the AI system lifecyle e.g.)</li><li>Stop the AI system in case of potential harm</li></ul><li>AI contributors should ease human intervention, according to one or more of the following approaches:<ul><li>Human-in-command (HIC)</li><li>Human-in-the-loop (HITL)</li><li>Human-on-the-loop (HOTL)*</li></ul></ul>|

<b>* Definitions</b> (inspired by EU):
- Human-in-the-loop (HITL): human intervention in every decision of the AI system
- Human-on-the-loop (HOTL): human intervention during the design of the AI system, and while the AI system operates, through monitoring e.g.
- Human-in-command (HIC): humans oversee the AI system and its implications, with the possibility to decide when and how to use it

## Risk & criticality

AI contributors must evaluate the risk and criticality of an AI system before starting to implement it, namely during ideation phases, among following risks:

- **Unacceptable Risk**: the AI system must be banned
- **High Risk**: the AI system must comply with regulation enforcements (see recommendations below)
- **Limited Risk**: the AI system must be transparent for the user
- **Minimal Risk**: the AI system is not subject to regulation

<details>
    <summary>Definitions (<b>click to unfold</b>)</summary>

Directly quoted from <a href="https://ec.europa.eu/commission/presscorner/detail/en/IP_21_1682">ec.europa.eu</a>

<ul>
  <li><b>Unacceptable risk</b>: <i>AI systems considered a clear threat to the safety, livelihoods and rights of people [...]. This includes AI systems or applications that manipulate human behaviour to circumvent users' free will (e.g. toys using voice assistance encouraging dangerous behaviour of minors) and systems that allow ‘social scoring' by governments.</i></li>
  <li><b>High-risk AI systems include</b>:
    <ul>
      <li><i>Critical infrastructures (e.g. transport), that could put the life and health of citizens at risk</i></li>
      <li><i>Educational or vocational training, that may determine the access to education and professional course of someone's life (e.g. scoring of exams); safety components of products (e.g. AI application in robot-assisted surgery)</i></li>
      <li><i>Employment, workers management and access to self-employment (e.g. CV-sorting software for recruitment procedures);</i></li>
      <li><i>Essential private and public services (e.g. credit scoring denying citizens opportunity to obtain a loan);</i></li>
      <li><i>Law enforcement that may interfere with people's fundamental rights (e.g. evaluation of the reliability of evidence)</i></li>
      <li><i>Migration, asylum and border control management (e.g. verification of authenticity of travel documents);</i></li>
      <li><i>Administration of justice and democratic processes (e.g. applying the law to a concrete set of facts)</i></li>
    </ul>
  <li><b>Limited risk</b><i> - AI system with transparency obligations: When using AI systems such as chatbots, users should be aware that they are interacting with a machine so they can take an informed decision to continue or step back.</i></li>
  <li><b>Minimal risk</b><i>: the legal proposal allows the free use of applications such as AI-enabled video games or spam filters. The vast majority of AI systems fall into this category. The draft Regulation does not intervene here, as these AI systems represent only minimal or no risk for citizens' rights or safety.</i></li>
</ul>
</details>

## Appendix - Recommendations from the EU
Below are the recommendations directly reported from [EU](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).

.. image:: ./_static/human_1.png
    :width: 60%
    :align: center

.. image:: ./_static/human_2.png
    :width: 60%
    :align: center

-