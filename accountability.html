<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Accountability &#8212; Trustworthy AI  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bibliography" href="bibliography.html" />
    <link rel="prev" title="Environmental and societal well-being" href="environmentalsocietal.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          Trustworthy AI</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Documentation <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="human.html">Human oversight</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacydatagov.html">Privacy and data governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="robustnesssafety.html">Technical robustness and security</a></li>
<li class="toctree-l1"><a class="reference internal" href="transparencyexplicability.html">Transparency and explicability</a></li>
<li class="toctree-l1"><a class="reference internal" href="diversitynondiscrimination.html">Diversity, non discrimination and fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="environmentalsocietal.html">Environmental and societal well-being</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Accountability</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Accountability</a><ul>
<li><a class="reference internal" href="#ai-contributors-are-responsible">AI contributors are responsible</a></li>
<li><a class="reference internal" href="#evaluation-by-internal-and-external-auditors">Evaluation by internal and external auditors</a></li>
<li><a class="reference internal" href="#identification-of-negative-impacts">Identification of negative impacts</a></li>
<li><a class="reference internal" href="#ensure-redress-is-possible">Ensure redress is possible</a></li>
<li><a class="reference internal" href="#appendix-recommendations-from-the-eu">Appendix - Recommendations from the EU</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="environmentalsocietal.html" title="Previous Chapter: Environmental and societal well-being"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Environmental...</span>
    </a>
  </li>
  <li>
    <a href="bibliography.html" title="Next Chapter: Bibliography"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Bibliography &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="accountability">
<h1>Accountability<a class="headerlink" href="#accountability" title="Permalink to this headline">¶</a></h1>
<p>Mechanisms must be put in place to ensure responsibility and accountability for AI systems and their outcomes, both before and after their deployment.</p>
<section id="ai-contributors-are-responsible">
<h2>AI contributors are responsible<a class="headerlink" href="#ai-contributors-are-responsible" title="Permalink to this headline">¶</a></h2>
<p>AI technologies can not be held responsible for their actions as they do not meet <span class="raw-html-m2r"><a href="https://link.springer.com/article/10.1007%2Fs13347-017-0285-z#Sec10">traditional criteria</a></span> for full moral action, such as consciousness and freedom.</p>
<p>AI systems can not act freely, as there is human involvement in the creation, usage or monitoring of an AI system (in particular: developers, business and AI users). Consequently, these people are (legally and morally) responsible for the actions of an AI system.</p>
</section>
<section id="evaluation-by-internal-and-external-auditors">
<h2>Evaluation by internal and external auditors<a class="headerlink" href="#evaluation-by-internal-and-external-auditors" title="Permalink to this headline">¶</a></h2>
<p>Evaluation by internal (within the company) and external (specialized agency) auditors and the availability of evaluation reports contribute to trustworthiness of AI. However, information about business models and intellectual property related to AI systems do not necessarily need to be open publicly. But in applications affecting fundamental rights, including safety critical applications, AI systems should be able to be independently audited.</p>
<p>As a starting point for auditing AI, one could leverage <a class="reference external" href="https://community.mis.temple.edu/mis5203sec001sp2019/files/2019/01/COBIT-2019-Framework-Introduction-and-Methodology_res_eng_1118.pdf">ISACA’s COBIT (2019)</a> framework. Explanation of how to apply this framework for AI projects (including for example metrics) can be found in this <a class="reference external" href="https://ec.europa.eu/futurium/en/system/files/ged/auditing-artificial-intelligence.pdf">document</a>.</p>
<p><span class="raw-html-m2r"><i> For more information on this subject, in particular regarding security and risks, one could also have a look at the 'security' section of the <a href="https://datacraft-paris.github.io/trustworthyai/robustnesssafety.html">technical robustness and security pillar</a>. </i></span></p>
</section>
<section id="identification-of-negative-impacts">
<h2>Identification of negative impacts<a class="headerlink" href="#identification-of-negative-impacts" title="Permalink to this headline">¶</a></h2>
<p>Identifying, assessing, documenting and minimising potential negative impacts of an AI system is crucial for people (in)directly affected by AI systems. Impact assessments can be helpful to minimize negative impact. Assessments must be proportionate to the risks AI systems pose.</p>
<p>Some examples of negative impacts of AI are the following:</p>
<ul class="simple">
<li><p>Built in biases, as it is built by humans it can have a conscious or unconscious bias.</p></li>
<li><p>Some jobs might be lost because tasks are replaced by AI.</p></li>
<li><p>Change in human experience, loss of personal experience/social interaction.</p></li>
<li><p>Acceleration of threats to cyber security.</p></li>
<li><p>Privacy/data breach or unlawful use of data.</p></li>
<li><p>Human no longer in (direct) control to overlook situations.</p></li>
<li><p>Social/wealth polarization.</p></li>
</ul>
<p><span class="raw-html-m2r"><i> For more information on this subject, one could also have a look at the 'risk and criticality' section of the <a href="https://datacraft-paris.github.io/trustworthyai/robustnesssafety.html">technical robustness and security pillar</a>. </i></span></p>
</section>
<section id="ensure-redress-is-possible">
<h2>Ensure redress is possible<a class="headerlink" href="#ensure-redress-is-possible" title="Permalink to this headline">¶</a></h2>
<p>When unjust adverse impact occurs, accessible mechanisms should be foreseen to ensure redress. National Human Rights structures (i.e. Ombudsmen and National Human Rights Institutions) can be such a source of redress, through rendering their own decisions in accordance with their respective mandates.</p>
<p>Knowing that redress is possible when things go wrong is key to ensure trust.</p>
</section>
<section id="appendix-recommendations-from-the-eu">
<h2>Appendix - Recommendations from the EU<a class="headerlink" href="#appendix-recommendations-from-the-eu" title="Permalink to this headline">¶</a></h2>
<p>Below are the recommendations directly reported from <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">EU</a>.</p>
<a class="reference internal image-reference" href="_images/accountability_1.png"><img alt="_images/accountability_1.png" class="align-center" src="_images/accountability_1.png" style="width: 60%;" /></a>
<ul class="simple">
<li></li>
</ul>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright Danone, Datacraft, 2021.<br/>
    </p>
  </div>
</footer>
  </body>
</html>